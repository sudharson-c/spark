1. Open ubuntu
2. Open a terminal
3. sudo apt-get install default-jdk scala -y
4. Check if scala is installed, 
	scala -version
5. Download spark using command,
wget https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz

6. Download spark-bin using command,
wget https://downloads.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz.sha512

7 Unzip the files using,
	.tar xvf spark-*.tgz

8. Now going to directory, 
	sudo mv spark-3.5.3-bin-hadoop3 /opt/spark
9. Checking for spark,
	/opt/spark/bin/spark-shell --version

10. Exporting the paths,

echo "export SPARK_HOME=/opt/spark" >> ~/.bashrc
echo "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin" >> ~/.bashrc
echo "export PYSPARK_PYTHON=/usr/bin/python3" >> ~/.bashrc

source ~/.bashrc

11. Opening spark shell,
	spark-shell

12. In spark shell,
	val inputfile = sc.textFile(“input.txt”)
	val counts = inputfile.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey(_+_);
	counts.toDebugString
	counts.cache()
	counts.saveAsTextFile("output")

13. In new terminal,
	cd output/
	cat part-00000
